{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Morsi Store\n",
      "[nltk_data]     DZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Morsi Store\n",
      "[nltk_data]     DZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Morsi Store\n",
      "[nltk_data]     DZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    def is_english_word(word):\n",
    "        # synsets = synonyms sets\n",
    "        synsets = wordnet.synsets(word)\n",
    "        # check if the given word is equal to the noun of lemmatized version of synonyms sets\n",
    "        return len(synsets) > 0 and synsets[0].lemmas()[0].name().lower() == word.lower()\n",
    "\n",
    "    # remove non-word characters and numbers using regular expressions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # remove English stop words\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in english_stopwords]\n",
    "    \n",
    "    # filter out non-English words\n",
    "    english_words = [token for token in filtered_tokens if is_english_word(token)]\n",
    "    \n",
    "    return english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m \u001b[43mWordNetLemmatizer\u001b[49m\n\u001b[0;32m      2\u001b[0m train_light \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_light.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m      4\u001b[0m words \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer\n",
    "train_light = json.loads(open('train_light.json').read())\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "train_light.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
